# Systems Programming and Concurrency with Go and Claude-Flow

Master Go's concurrent programming patterns and systems-level development with claude-flow assistance.

## 🧵 Concurrency Fundamentals

### Goroutines and Channels
```bash
# Generate concurrent programming examples
npx claude-flow sparc run architect "Go concurrency patterns with goroutines and channels"
```

**Generated Concurrency Patterns:**
```go
// internal/concurrency/patterns.go - Generated by claude-flow
package concurrency

import (
    "context"
    "fmt"
    "sync"
    "time"
)

// Worker Pool Pattern
type WorkerPool struct {
    workerCount int
    jobs        chan Job
    results     chan Result
    wg          sync.WaitGroup
}

type Job struct {
    ID   int
    Data interface{}
}

type Result struct {
    Job   Job
    Value interface{}
    Error error
}

func NewWorkerPool(workerCount int) *WorkerPool {
    return &WorkerPool{
        workerCount: workerCount,
        jobs:        make(chan Job, workerCount*2),
        results:     make(chan Result, workerCount*2),
    }
}

func (wp *WorkerPool) Start(ctx context.Context) {
    // Start workers
    for i := 0; i < wp.workerCount; i++ {
        wp.wg.Add(1)
        go wp.worker(ctx, i)
    }

    // Start result collector
    go wp.collectResults(ctx)
}

func (wp *WorkerPool) worker(ctx context.Context, id int) {
    defer wp.wg.Done()

    for {
        select {
        case job := <-wp.jobs:
            result := wp.processJob(job)
            wp.results <- result
        case <-ctx.Done():
            fmt.Printf("Worker %d shutting down\n", id)
            return
        }
    }
}

func (wp *WorkerPool) processJob(job Job) Result {
    // Simulate work
    time.Sleep(time.Millisecond * 100)

    return Result{
        Job:   job,
        Value: fmt.Sprintf("Processed job %d", job.ID),
        Error: nil,
    }
}

func (wp *WorkerPool) collectResults(ctx context.Context) {
    for {
        select {
        case result := <-wp.results:
            fmt.Printf("Result: %v\n", result.Value)
        case <-ctx.Done():
            return
        }
    }
}

func (wp *WorkerPool) Submit(job Job) {
    wp.jobs <- job
}

func (wp *WorkerPool) Shutdown() {
    close(wp.jobs)
    wp.wg.Wait()
    close(wp.results)
}

// Fan-in/Fan-out Pattern
func FanOut(input <-chan int, workers int) []<-chan int {
    outputs := make([]<-chan int, workers)

    for i := 0; i < workers; i++ {
        output := make(chan int)
        outputs[i] = output

        go func(out chan<- int) {
            defer close(out)
            for data := range input {
                // Process data
                out <- data * 2
            }
        }(output)
    }

    return outputs
}

func FanIn(inputs ...<-chan int) <-chan int {
    output := make(chan int)
    var wg sync.WaitGroup

    wg.Add(len(inputs))

    for _, input := range inputs {
        go func(ch <-chan int) {
            defer wg.Done()
            for data := range ch {
                output <- data
            }
        }(input)
    }

    go func() {
        wg.Wait()
        close(output)
    }()

    return output
}

// Pipeline Pattern
type Pipeline struct {
    stages []func(<-chan interface{}) <-chan interface{}
}

func NewPipeline() *Pipeline {
    return &Pipeline{}
}

func (p *Pipeline) AddStage(stage func(<-chan interface{}) <-chan interface{}) {
    p.stages = append(p.stages, stage)
}

func (p *Pipeline) Execute(input <-chan interface{}) <-chan interface{} {
    output := input

    for _, stage := range p.stages {
        output = stage(output)
    }

    return output
}

// Rate Limiter Pattern
type RateLimiter struct {
    tokens chan struct{}
    ticker *time.Ticker
}

func NewRateLimiter(rate time.Duration, burst int) *RateLimiter {
    rl := &RateLimiter{
        tokens: make(chan struct{}, burst),
        ticker: time.NewTicker(rate),
    }

    // Fill initial tokens
    for i := 0; i < burst; i++ {
        rl.tokens <- struct{}{}
    }

    // Refill tokens
    go func() {
        for range rl.ticker.C {
            select {
            case rl.tokens <- struct{}{}:
            default:
                // Token bucket is full
            }
        }
    }()

    return rl
}

func (rl *RateLimiter) Allow() bool {
    select {
    case <-rl.tokens:
        return true
    default:
        return false
    }
}

func (rl *RateLimiter) Wait(ctx context.Context) error {
    select {
    case <-rl.tokens:
        return nil
    case <-ctx.Done():
        return ctx.Err()
    }
}

func (rl *RateLimiter) Stop() {
    rl.ticker.Stop()
}
```

## 🔄 Advanced Concurrency Patterns

### Context and Cancellation
```go
// internal/concurrency/context.go - Generated by claude-flow
package concurrency

import (
    "context"
    "fmt"
    "time"
)

// Timeout Handler
func WithTimeout(ctx context.Context, timeout time.Duration, fn func(context.Context) error) error {
    ctx, cancel := context.WithTimeout(ctx, timeout)
    defer cancel()

    done := make(chan error, 1)

    go func() {
        done <- fn(ctx)
    }()

    select {
    case err := <-done:
        return err
    case <-ctx.Done():
        return ctx.Err()
    }
}

// Cancellable Operation
type CancellableOperation struct {
    ctx    context.Context
    cancel context.CancelFunc
}

func NewCancellableOperation(parent context.Context) *CancellableOperation {
    ctx, cancel := context.WithCancel(parent)
    return &CancellableOperation{
        ctx:    ctx,
        cancel: cancel,
    }
}

func (op *CancellableOperation) Execute(work func(context.Context) error) error {
    return work(op.ctx)
}

func (op *CancellableOperation) Cancel() {
    op.cancel()
}

// Circuit Breaker Pattern
type CircuitBreaker struct {
    maxFailures int
    resetTime   time.Duration
    failures    int
    lastFailure time.Time
    state       string // "closed", "open", "half-open"
    mutex       sync.RWMutex
}

func NewCircuitBreaker(maxFailures int, resetTime time.Duration) *CircuitBreaker {
    return &CircuitBreaker{
        maxFailures: maxFailures,
        resetTime:   resetTime,
        state:       "closed",
    }
}

func (cb *CircuitBreaker) Call(operation func() error) error {
    cb.mutex.Lock()
    defer cb.mutex.Unlock()

    if cb.state == "open" {
        if time.Since(cb.lastFailure) > cb.resetTime {
            cb.state = "half-open"
            cb.failures = 0
        } else {
            return fmt.Errorf("circuit breaker is open")
        }
    }

    err := operation()

    if err != nil {
        cb.failures++
        cb.lastFailure = time.Now()

        if cb.failures >= cb.maxFailures {
            cb.state = "open"
        }

        return err
    }

    // Success
    cb.failures = 0
    cb.state = "closed"
    return nil
}

// Bulkhead Pattern for Resource Isolation
type Bulkhead struct {
    semaphores map[string]chan struct{}
    mutex      sync.RWMutex
}

func NewBulkhead() *Bulkhead {
    return &Bulkhead{
        semaphores: make(map[string]chan struct{}),
    }
}

func (b *Bulkhead) AddResource(name string, limit int) {
    b.mutex.Lock()
    defer b.mutex.Unlock()

    b.semaphores[name] = make(chan struct{}, limit)

    // Fill semaphore
    for i := 0; i < limit; i++ {
        b.semaphores[name] <- struct{}{}
    }
}

func (b *Bulkhead) Acquire(resource string, ctx context.Context) error {
    b.mutex.RLock()
    sem, exists := b.semaphores[resource]
    b.mutex.RUnlock()

    if !exists {
        return fmt.Errorf("resource %s not found", resource)
    }

    select {
    case <-sem:
        return nil
    case <-ctx.Done():
        return ctx.Err()
    }
}

func (b *Bulkhead) Release(resource string) error {
    b.mutex.RLock()
    sem, exists := b.semaphores[resource]
    b.mutex.RUnlock()

    if !exists {
        return fmt.Errorf("resource %s not found", resource)
    }

    select {
    case sem <- struct{}{}:
        return nil
    default:
        return fmt.Errorf("resource %s semaphore is full", resource)
    }
}
```

## 🔧 Systems Programming

### Memory Management and Optimization
```bash
# Generate memory-efficient patterns
npx claude-flow sparc run perf-analyzer "Go memory optimization patterns and techniques"
```

**Generated Memory Management Code:**
```go
// internal/memory/pool.go - Generated by claude-flow
package memory

import (
    "sync"
)

// Object Pool Pattern
type Pool struct {
    pool sync.Pool
    new  func() interface{}
}

func NewPool(new func() interface{}) *Pool {
    return &Pool{
        pool: sync.Pool{New: new},
        new:  new,
    }
}

func (p *Pool) Get() interface{} {
    return p.pool.Get()
}

func (p *Pool) Put(obj interface{}) {
    p.pool.Put(obj)
}

// Buffer Pool for reducing allocations
type BufferPool struct {
    pool sync.Pool
}

func NewBufferPool() *BufferPool {
    return &BufferPool{
        pool: sync.Pool{
            New: func() interface{} {
                return make([]byte, 0, 1024) // 1KB initial capacity
            },
        },
    }
}

func (bp *BufferPool) GetBuffer() []byte {
    return bp.pool.Get().([]byte)[:0] // Reset length but keep capacity
}

func (bp *BufferPool) PutBuffer(buf []byte) {
    if cap(buf) > 64*1024 { // Don't pool very large buffers
        return
    }
    bp.pool.Put(buf)
}

// String Builder Pool
type StringBuilderPool struct {
    pool sync.Pool
}

func NewStringBuilderPool() *StringBuilderPool {
    return &StringBuilderPool{
        pool: sync.Pool{
            New: func() interface{} {
                return &strings.Builder{}
            },
        },
    }
}

func (sbp *StringBuilderPool) Get() *strings.Builder {
    return sbp.pool.Get().(*strings.Builder)
}

func (sbp *StringBuilderPool) Put(sb *strings.Builder) {
    sb.Reset()
    sbp.pool.Put(sb)
}

// Memory Monitor
type MemoryMonitor struct {
    interval time.Duration
    callback func(stats MemoryStats)
    stop     chan struct{}
}

type MemoryStats struct {
    Alloc        uint64
    TotalAlloc   uint64
    Sys          uint64
    NumGC        uint32
    GCPauseTotal time.Duration
}

func NewMemoryMonitor(interval time.Duration, callback func(MemoryStats)) *MemoryMonitor {
    return &MemoryMonitor{
        interval: interval,
        callback: callback,
        stop:     make(chan struct{}),
    }
}

func (mm *MemoryMonitor) Start() {
    ticker := time.NewTicker(mm.interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            var m runtime.MemStats
            runtime.ReadMemStats(&m)

            stats := MemoryStats{
                Alloc:        m.Alloc,
                TotalAlloc:   m.TotalAlloc,
                Sys:          m.Sys,
                NumGC:        m.NumGC,
                GCPauseTotal: time.Duration(m.PauseTotalNs),
            }

            mm.callback(stats)

        case <-mm.stop:
            return
        }
    }
}

func (mm *MemoryMonitor) Stop() {
    close(mm.stop)
}
```

### File System and I/O Operations
```go
// internal/io/filesystem.go - Generated by claude-flow
package io

import (
    "bufio"
    "context"
    "io"
    "os"
    "path/filepath"
    "sync"
)

// Concurrent File Processor
type FileProcessor struct {
    workerCount int
    bufferSize  int
}

func NewFileProcessor(workerCount, bufferSize int) *FileProcessor {
    return &FileProcessor{
        workerCount: workerCount,
        bufferSize:  bufferSize,
    }
}

func (fp *FileProcessor) ProcessDirectory(ctx context.Context, dir string, processor func(string, []byte) error) error {
    files := make(chan string, fp.workerCount*2)
    errors := make(chan error, fp.workerCount)

    var wg sync.WaitGroup

    // Start workers
    for i := 0; i < fp.workerCount; i++ {
        wg.Add(1)
        go fp.worker(ctx, &wg, files, errors, processor)
    }

    // Walk directory and send files to workers
    go func() {
        defer close(files)

        err := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {
            if err != nil {
                return err
            }

            if !info.IsDir() {
                select {
                case files <- path:
                case <-ctx.Done():
                    return ctx.Err()
                }
            }

            return nil
        })

        if err != nil {
            errors <- err
        }
    }()

    // Wait for workers to finish
    go func() {
        wg.Wait()
        close(errors)
    }()

    // Collect errors
    for err := range errors {
        if err != nil {
            return err
        }
    }

    return nil
}

func (fp *FileProcessor) worker(ctx context.Context, wg *sync.WaitGroup, files <-chan string, errors chan<- error, processor func(string, []byte) error) {
    defer wg.Done()

    buffer := make([]byte, fp.bufferSize)

    for {
        select {
        case filename, ok := <-files:
            if !ok {
                return
            }

            if err := fp.processFile(filename, buffer, processor); err != nil {
                errors <- err
                return
            }

        case <-ctx.Done():
            return
        }
    }
}

func (fp *FileProcessor) processFile(filename string, buffer []byte, processor func(string, []byte) error) error {
    file, err := os.Open(filename)
    if err != nil {
        return err
    }
    defer file.Close()

    reader := bufio.NewReader(file)

    for {
        n, err := reader.Read(buffer)
        if n > 0 {
            if err := processor(filename, buffer[:n]); err != nil {
                return err
            }
        }

        if err == io.EOF {
            break
        }
        if err != nil {
            return err
        }
    }

    return nil
}

// Streaming File Writer
type StreamWriter struct {
    file   *os.File
    writer *bufio.Writer
    mutex  sync.Mutex
}

func NewStreamWriter(filename string) (*StreamWriter, error) {
    file, err := os.Create(filename)
    if err != nil {
        return nil, err
    }

    return &StreamWriter{
        file:   file,
        writer: bufio.NewWriter(file),
    }, nil
}

func (sw *StreamWriter) Write(data []byte) error {
    sw.mutex.Lock()
    defer sw.mutex.Unlock()

    _, err := sw.writer.Write(data)
    return err
}

func (sw *StreamWriter) Flush() error {
    sw.mutex.Lock()
    defer sw.mutex.Unlock()

    return sw.writer.Flush()
}

func (sw *StreamWriter) Close() error {
    sw.mutex.Lock()
    defer sw.mutex.Unlock()

    if err := sw.writer.Flush(); err != nil {
        return err
    }

    return sw.file.Close()
}
```

## 🌐 Network Programming

### TCP/UDP Servers and Clients
```bash
# Generate network programming examples
npx claude-flow sparc run architect "Go network programming with TCP and UDP servers"
```

**Generated Network Code:**
```go
// internal/network/tcp_server.go - Generated by claude-flow
package network

import (
    "context"
    "fmt"
    "net"
    "sync"
    "time"
)

type TCPServer struct {
    address string
    handler ConnectionHandler
    listener net.Listener
    connections map[net.Conn]bool
    mutex      sync.RWMutex
    wg         sync.WaitGroup
}

type ConnectionHandler interface {
    Handle(conn net.Conn) error
}

func NewTCPServer(address string, handler ConnectionHandler) *TCPServer {
    return &TCPServer{
        address:     address,
        handler:     handler,
        connections: make(map[net.Conn]bool),
    }
}

func (s *TCPServer) Start(ctx context.Context) error {
    listener, err := net.Listen("tcp", s.address)
    if err != nil {
        return fmt.Errorf("failed to listen on %s: %w", s.address, err)
    }

    s.listener = listener

    go s.acceptConnections(ctx)

    return nil
}

func (s *TCPServer) acceptConnections(ctx context.Context) {
    for {
        conn, err := s.listener.Accept()
        if err != nil {
            select {
            case <-ctx.Done():
                return
            default:
                fmt.Printf("Accept error: %v\n", err)
                continue
            }
        }

        s.addConnection(conn)

        s.wg.Add(1)
        go s.handleConnection(ctx, conn)
    }
}

func (s *TCPServer) handleConnection(ctx context.Context, conn net.Conn) {
    defer s.wg.Done()
    defer s.removeConnection(conn)
    defer conn.Close()

    // Set connection deadline
    conn.SetDeadline(time.Now().Add(30 * time.Second))

    if err := s.handler.Handle(conn); err != nil {
        fmt.Printf("Connection handler error: %v\n", err)
    }
}

func (s *TCPServer) addConnection(conn net.Conn) {
    s.mutex.Lock()
    defer s.mutex.Unlock()
    s.connections[conn] = true
}

func (s *TCPServer) removeConnection(conn net.Conn) {
    s.mutex.Lock()
    defer s.mutex.Unlock()
    delete(s.connections, conn)
}

func (s *TCPServer) Stop(ctx context.Context) error {
    if s.listener != nil {
        s.listener.Close()
    }

    // Close all connections
    s.mutex.RLock()
    for conn := range s.connections {
        conn.Close()
    }
    s.mutex.RUnlock()

    // Wait for all connections to finish
    done := make(chan struct{})
    go func() {
        s.wg.Wait()
        close(done)
    }()

    select {
    case <-done:
        return nil
    case <-ctx.Done():
        return ctx.Err()
    }
}

// Connection Pool
type ConnectionPool struct {
    network   string
    address   string
    minConns  int
    maxConns  int
    conns     chan net.Conn
    active    int
    mutex     sync.Mutex
}

func NewConnectionPool(network, address string, minConns, maxConns int) *ConnectionPool {
    pool := &ConnectionPool{
        network:  network,
        address:  address,
        minConns: minConns,
        maxConns: maxConns,
        conns:    make(chan net.Conn, maxConns),
    }

    // Create initial connections
    for i := 0; i < minConns; i++ {
        if conn, err := net.Dial(network, address); err == nil {
            pool.conns <- conn
            pool.active++
        }
    }

    return pool
}

func (p *ConnectionPool) Get() (net.Conn, error) {
    select {
    case conn := <-p.conns:
        p.mutex.Lock()
        p.active--
        p.mutex.Unlock()
        return conn, nil
    default:
        // Create new connection if under limit
        p.mutex.Lock()
        if p.active < p.maxConns {
            p.active++
            p.mutex.Unlock()
            return net.Dial(p.network, p.address)
        }
        p.mutex.Unlock()

        // Wait for available connection
        conn := <-p.conns
        return conn, nil
    }
}

func (p *ConnectionPool) Put(conn net.Conn) {
    select {
    case p.conns <- conn:
        p.mutex.Lock()
        p.active++
        p.mutex.Unlock()
    default:
        // Pool is full, close connection
        conn.Close()
    }
}

func (p *ConnectionPool) Close() {
    close(p.conns)
    for conn := range p.conns {
        conn.Close()
    }
}
```

## 🧪 Testing Concurrent Code

### Race Detection and Testing
```bash
# Generate concurrent testing examples
npx claude-flow sparc run tester "Go concurrent code testing with race detection"
```

**Generated Test Code:**
```go
// internal/concurrency/patterns_test.go - Generated by claude-flow
package concurrency

import (
    "context"
    "sync"
    "testing"
    "time"
)

func TestWorkerPool(t *testing.T) {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    pool := NewWorkerPool(3)
    pool.Start(ctx)

    // Submit jobs
    for i := 0; i < 10; i++ {
        pool.Submit(Job{ID: i, Data: fmt.Sprintf("job-%d", i)})
    }

    // Wait a bit for processing
    time.Sleep(2 * time.Second)

    pool.Shutdown()
}

func TestRateLimiter(t *testing.T) {
    limiter := NewRateLimiter(100*time.Millisecond, 5)
    defer limiter.Stop()

    // Test burst capacity
    allowed := 0
    for i := 0; i < 10; i++ {
        if limiter.Allow() {
            allowed++
        }
    }

    if allowed != 5 {
        t.Errorf("Expected 5 allowed requests, got %d", allowed)
    }

    // Test rate limiting
    time.Sleep(150 * time.Millisecond)
    if !limiter.Allow() {
        t.Error("Expected request to be allowed after waiting")
    }
}

func TestConcurrentAccess(t *testing.T) {
    const goroutines = 100
    const iterations = 1000

    var counter int64
    var wg sync.WaitGroup

    wg.Add(goroutines)

    for i := 0; i < goroutines; i++ {
        go func() {
            defer wg.Done()
            for j := 0; j < iterations; j++ {
                atomic.AddInt64(&counter, 1)
            }
        }()
    }

    wg.Wait()

    expected := int64(goroutines * iterations)
    if counter != expected {
        t.Errorf("Expected counter to be %d, got %d", expected, counter)
    }
}

// Benchmark concurrent operations
func BenchmarkWorkerPool(b *testing.B) {
    ctx := context.Background()
    pool := NewWorkerPool(runtime.NumCPU())
    pool.Start(ctx)
    defer pool.Shutdown()

    b.ResetTimer()

    for i := 0; i < b.N; i++ {
        pool.Submit(Job{ID: i, Data: i})
    }
}

func BenchmarkChannelOperations(b *testing.B) {
    ch := make(chan int, 1000)

    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            select {
            case ch <- 1:
            case <-ch:
            default:
            }
        }
    })
}

// Property-based testing
func TestConcurrencyProperties(t *testing.T) {
    if testing.Short() {
        t.Skip("Skipping property test in short mode")
    }

    // Test that worker pool doesn't lose jobs
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()

    pool := NewWorkerPool(5)
    pool.Start(ctx)

    jobCount := 1000
    results := make(map[int]bool)
    var mutex sync.Mutex

    // Custom result collector
    go func() {
        for {
            select {
            case result := <-pool.results:
                mutex.Lock()
                results[result.Job.ID] = true
                mutex.Unlock()
            case <-ctx.Done():
                return
            }
        }
    }()

    // Submit jobs
    for i := 0; i < jobCount; i++ {
        pool.Submit(Job{ID: i, Data: i})
    }

    // Wait for completion
    time.Sleep(5 * time.Second)

    mutex.Lock()
    processedCount := len(results)
    mutex.Unlock()

    if processedCount != jobCount {
        t.Errorf("Expected %d processed jobs, got %d", jobCount, processedCount)
    }

    pool.Shutdown()
}
```

## 🚀 Claude-Flow Commands for Systems Programming

```bash
# Generate concurrent system
npx claude-flow sparc run architect "concurrent file processing system with worker pools"

# Optimize performance
npx claude-flow sparc run perf-analyzer "optimize goroutine usage and memory allocation"

# Add monitoring
npx claude-flow sparc run coder "add metrics and monitoring for concurrent operations"

# Security review
npx claude-flow sparc run security-auditor "review concurrent code for race conditions"

# Generate benchmarks
npx claude-flow sparc run tester "create benchmarks for concurrent operations"
```

## 📈 Performance Considerations

### Best Practices for Concurrent Go
1. **Use Buffered Channels**: Prevent goroutine blocking
2. **Pool Resources**: Reuse expensive objects
3. **Context Propagation**: Proper cancellation handling
4. **Avoid Premature Optimization**: Profile first
5. **Monitor Memory**: Watch for goroutine leaks

### Common Pitfalls
- **Goroutine Leaks**: Always ensure goroutines can exit
- **Channel Deadlocks**: Careful with unbuffered channels
- **Race Conditions**: Use race detector in testing
- **Resource Exhaustion**: Limit concurrent operations

**Next Steps:**
- [Testing Strategies](../testing/) - Test concurrent code effectively
- [Performance](../performance/) - Profile and optimize Go applications
- [Examples](../examples/) - See complete concurrent systems