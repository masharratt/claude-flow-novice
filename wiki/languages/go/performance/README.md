# Go Performance Optimization and Profiling with Claude-Flow

Advanced performance optimization techniques for Go applications using claude-flow for automated analysis and optimization.

## âš¡ Performance Analysis Overview

### Quick Performance Assessment
```bash
# Generate performance analysis with claude-flow
npx claude-flow sparc run perf-analyzer "comprehensive Go application performance analysis"

# Automated bottleneck detection
npx claude-flow sparc run perf-analyzer "identify and fix performance bottlenecks in concurrent Go application"
```

## ðŸ“Š Profiling Fundamentals

### Built-in Profiling Tools
```go
// internal/profiling/profiler.go - Generated by claude-flow
package profiling

import (
    "context"
    "fmt"
    "log"
    "net/http"
    _ "net/http/pprof"
    "os"
    "runtime"
    "runtime/pprof"
    "runtime/trace"
    "time"
)

type Profiler struct {
    cpuProfile   *os.File
    memProfile   *os.File
    traceFile    *os.File
    pprofServer  *http.Server
}

func NewProfiler() *Profiler {
    return &Profiler{}
}

// Start HTTP profiling server
func (p *Profiler) StartHTTPProfiler(addr string) error {
    mux := http.NewServeMux()
    mux.HandleFunc("/debug/pprof/", http.DefaultServeMux.ServeHTTP)

    p.pprofServer = &http.Server{
        Addr:    addr,
        Handler: mux,
    }

    go func() {
        log.Printf("Profiling server started on %s", addr)
        if err := p.pprofServer.ListenAndServe(); err != http.ErrServerClosed {
            log.Printf("Profiling server error: %v", err)
        }
    }()

    return nil
}

// Start CPU profiling
func (p *Profiler) StartCPUProfile(filename string) error {
    f, err := os.Create(filename)
    if err != nil {
        return fmt.Errorf("could not create CPU profile: %w", err)
    }

    if err := pprof.StartCPUProfile(f); err != nil {
        f.Close()
        return fmt.Errorf("could not start CPU profile: %w", err)
    }

    p.cpuProfile = f
    log.Printf("CPU profiling started, writing to %s", filename)
    return nil
}

// Stop CPU profiling
func (p *Profiler) StopCPUProfile() {
    if p.cpuProfile != nil {
        pprof.StopCPUProfile()
        p.cpuProfile.Close()
        p.cpuProfile = nil
        log.Println("CPU profiling stopped")
    }
}

// Write memory profile
func (p *Profiler) WriteMemProfile(filename string) error {
    f, err := os.Create(filename)
    if err != nil {
        return fmt.Errorf("could not create memory profile: %w", err)
    }
    defer f.Close()

    runtime.GC() // Force GC to get accurate profile
    if err := pprof.WriteHeapProfile(f); err != nil {
        return fmt.Errorf("could not write memory profile: %w", err)
    }

    log.Printf("Memory profile written to %s", filename)
    return nil
}

// Start execution trace
func (p *Profiler) StartTrace(filename string) error {
    f, err := os.Create(filename)
    if err != nil {
        return fmt.Errorf("could not create trace file: %w", err)
    }

    if err := trace.Start(f); err != nil {
        f.Close()
        return fmt.Errorf("could not start trace: %w", err)
    }

    p.traceFile = f
    log.Printf("Execution tracing started, writing to %s", filename)
    return nil
}

// Stop execution trace
func (p *Profiler) StopTrace() {
    if p.traceFile != nil {
        trace.Stop()
        p.traceFile.Close()
        p.traceFile = nil
        log.Println("Execution tracing stopped")
    }
}

// Profile a function execution
func (p *Profiler) ProfileFunction(name string, fn func()) {
    log.Printf("Starting profile for function: %s", name)

    cpuFile := fmt.Sprintf("cpu_%s_%d.prof", name, time.Now().Unix())
    memFile := fmt.Sprintf("mem_%s_%d.prof", name, time.Now().Unix())
    traceFile := fmt.Sprintf("trace_%s_%d.out", name, time.Now().Unix())

    // Start profiling
    p.StartCPUProfile(cpuFile)
    p.StartTrace(traceFile)

    start := time.Now()
    fn()
    duration := time.Since(start)

    // Stop profiling
    p.StopCPUProfile()
    p.StopTrace()
    p.WriteMemProfile(memFile)

    log.Printf("Function %s completed in %v", name, duration)
}

// Continuous memory monitoring
func (p *Profiler) MonitorMemory(ctx context.Context, interval time.Duration, callback func(MemoryStats)) {
    ticker := time.NewTicker(interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            var m runtime.MemStats
            runtime.ReadMemStats(&m)

            stats := MemoryStats{
                Alloc:         m.Alloc,
                TotalAlloc:    m.TotalAlloc,
                Sys:           m.Sys,
                NumGC:         m.NumGC,
                GCPauseTotal:  time.Duration(m.PauseTotalNs),
                HeapAlloc:     m.HeapAlloc,
                HeapSys:       m.HeapSys,
                HeapIdle:      m.HeapIdle,
                HeapInuse:     m.HeapInuse,
                StackInuse:    m.StackInuse,
                StackSys:      m.StackSys,
                MSpanInuse:    m.MSpanInuse,
                MSpanSys:      m.MSpanSys,
                MCacheInuse:   m.MCacheInuse,
                MCacheSys:     m.MCacheSys,
                NextGC:        m.NextGC,
                LastGC:        time.Unix(0, int64(m.LastGC)),
            }

            callback(stats)

        case <-ctx.Done():
            return
        }
    }
}

type MemoryStats struct {
    Alloc         uint64
    TotalAlloc    uint64
    Sys           uint64
    NumGC         uint32
    GCPauseTotal  time.Duration
    HeapAlloc     uint64
    HeapSys       uint64
    HeapIdle      uint64
    HeapInuse     uint64
    StackInuse    uint64
    StackSys      uint64
    MSpanInuse    uint64
    MSpanSys      uint64
    MCacheInuse   uint64
    MCacheSys     uint64
    NextGC        uint64
    LastGC        time.Time
}

// Cleanup resources
func (p *Profiler) Cleanup() {
    p.StopCPUProfile()
    p.StopTrace()

    if p.pprofServer != nil {
        ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
        defer cancel()
        p.pprofServer.Shutdown(ctx)
    }
}
```

### Automated Profiling Integration
```go
// internal/middleware/profiling.go - Generated by claude-flow
package middleware

import (
    "net/http"
    "runtime"
    "strconv"
    "time"

    "github.com/gin-gonic/gin"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    requestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "http_request_duration_seconds",
            Help: "Duration of HTTP requests",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint", "status"},
    )

    memoryUsage = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "memory_usage_bytes",
            Help: "Current memory usage in bytes",
        },
        []string{"type"},
    )

    goroutineCount = promauto.NewGauge(
        prometheus.GaugeOpts{
            Name: "goroutines_count",
            Help: "Number of goroutines",
        },
    )

    gcDuration = promauto.NewHistogram(
        prometheus.HistogramOpts{
            Name: "gc_duration_seconds",
            Help: "Time spent in garbage collection",
        },
    )
)

// Performance monitoring middleware
func PerformanceMonitoring() gin.HandlerFunc {
    // Start memory monitoring
    go func() {
        ticker := time.NewTicker(30 * time.Second)
        defer ticker.Stop()

        for range ticker.C {
            var m runtime.MemStats
            runtime.ReadMemStats(&m)

            memoryUsage.WithLabelValues("alloc").Set(float64(m.Alloc))
            memoryUsage.WithLabelValues("sys").Set(float64(m.Sys))
            memoryUsage.WithLabelValues("heap_alloc").Set(float64(m.HeapAlloc))
            memoryUsage.WithLabelValues("heap_sys").Set(float64(m.HeapSys))

            goroutineCount.Set(float64(runtime.NumGoroutine()))
            gcDuration.Observe(float64(m.PauseTotalNs) / 1e9)
        }
    }()

    return func(c *gin.Context) {
        start := time.Now()

        c.Next()

        duration := time.Since(start)
        status := strconv.Itoa(c.Writer.Status())

        requestDuration.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
            status,
        ).Observe(duration.Seconds())
    }
}

// Request-specific profiling
func RequestProfiler() gin.HandlerFunc {
    return func(c *gin.Context) {
        if c.Query("profile") == "true" {
            var m1, m2 runtime.MemStats
            runtime.ReadMemStats(&m1)

            start := time.Now()
            c.Next()
            duration := time.Since(start)

            runtime.ReadMemStats(&m2)

            c.Header("X-Profile-Duration", duration.String())
            c.Header("X-Profile-Memory-Alloc", strconv.FormatUint(m2.Alloc-m1.Alloc, 10))
            c.Header("X-Profile-Memory-Mallocs", strconv.FormatUint(m2.Mallocs-m1.Mallocs, 10))
        } else {
            c.Next()
        }
    }
}
```

## ðŸš€ Memory Optimization

### Object Pooling and Allocation Reduction
```go
// internal/optimization/memory.go - Generated by claude-flow
package optimization

import (
    "bytes"
    "encoding/json"
    "sync"
    "unsafe"
)

// Generic object pool
type Pool[T any] struct {
    pool sync.Pool
    new  func() T
}

func NewPool[T any](newFunc func() T) *Pool[T] {
    return &Pool[T]{
        pool: sync.Pool{
            New: func() interface{} {
                return newFunc()
            },
        },
        new: newFunc,
    }
}

func (p *Pool[T]) Get() T {
    return p.pool.Get().(T)
}

func (p *Pool[T]) Put(obj T) {
    p.pool.Put(obj)
}

// Specific pools for common types
var (
    BufferPool = &sync.Pool{
        New: func() interface{} {
            return bytes.NewBuffer(make([]byte, 0, 1024))
        },
    }

    SlicePool = &sync.Pool{
        New: func() interface{} {
            return make([]interface{}, 0, 100)
        },
    }

    MapPool = &sync.Pool{
        New: func() interface{} {
            return make(map[string]interface{}, 100)
        },
    }
)

// Buffer pool helpers
func GetBuffer() *bytes.Buffer {
    return BufferPool.Get().(*bytes.Buffer)
}

func PutBuffer(buf *bytes.Buffer) {
    buf.Reset()
    if buf.Cap() <= 64*1024 { // Don't pool very large buffers
        BufferPool.Put(buf)
    }
}

// JSON marshaling with pooled buffers
type JSONMarshaler struct {
    bufferPool *sync.Pool
}

func NewJSONMarshaler() *JSONMarshaler {
    return &JSONMarshaler{
        bufferPool: &sync.Pool{
            New: func() interface{} {
                return bytes.NewBuffer(make([]byte, 0, 512))
            },
        },
    }
}

func (jm *JSONMarshaler) Marshal(v interface{}) ([]byte, error) {
    buf := jm.bufferPool.Get().(*bytes.Buffer)
    defer func() {
        buf.Reset()
        jm.bufferPool.Put(buf)
    }()

    encoder := json.NewEncoder(buf)
    if err := encoder.Encode(v); err != nil {
        return nil, err
    }

    // Remove trailing newline added by Encode
    data := buf.Bytes()
    if len(data) > 0 && data[len(data)-1] == '\n' {
        data = data[:len(data)-1]
    }

    // Return a copy since buf will be reused
    result := make([]byte, len(data))
    copy(result, data)
    return result, nil
}

// String interning for memory efficiency
type StringInterner struct {
    intern map[string]string
    mutex  sync.RWMutex
}

func NewStringInterner() *StringInterner {
    return &StringInterner{
        intern: make(map[string]string),
    }
}

func (si *StringInterner) Intern(s string) string {
    si.mutex.RLock()
    if interned, exists := si.intern[s]; exists {
        si.mutex.RUnlock()
        return interned
    }
    si.mutex.RUnlock()

    si.mutex.Lock()
    defer si.mutex.Unlock()

    // Double-check after acquiring write lock
    if interned, exists := si.intern[s]; exists {
        return interned
    }

    // Make a copy to ensure we own the memory
    interned := string([]byte(s))
    si.intern[interned] = interned
    return interned
}

// Zero-allocation string to bytes conversion (unsafe)
func StringToBytes(s string) []byte {
    return *(*[]byte)(unsafe.Pointer(
        &struct {
            string
            Cap int
        }{s, len(s)},
    ))
}

// Zero-allocation bytes to string conversion (unsafe)
func BytesToString(b []byte) string {
    return *(*string)(unsafe.Pointer(&b))
}

// Memory-efficient slice operations
type SliceOptimizer struct{}

// Remove element without allocation
func (so *SliceOptimizer) RemoveUnordered(slice []interface{}, index int) []interface{} {
    slice[index] = slice[len(slice)-1]
    return slice[:len(slice)-1]
}

// Remove element preserving order
func (so *SliceOptimizer) RemoveOrdered(slice []interface{}, index int) []interface{} {
    copy(slice[index:], slice[index+1:])
    return slice[:len(slice)-1]
}

// Grow slice with optimal capacity
func (so *SliceOptimizer) GrowSlice(slice []interface{}, newSize int) []interface{} {
    if cap(slice) >= newSize {
        return slice[:newSize]
    }

    // Grow by 1.5x or to exact size, whichever is larger
    newCap := cap(slice) * 3 / 2
    if newCap < newSize {
        newCap = newSize
    }

    newSlice := make([]interface{}, newSize, newCap)
    copy(newSlice, slice)
    return newSlice
}

// Memory usage analyzer
type MemoryAnalyzer struct {
    samples []MemorySample
    mutex   sync.Mutex
}

type MemorySample struct {
    Timestamp time.Time
    Alloc     uint64
    HeapAlloc uint64
    Sys       uint64
    NumGC     uint32
}

func NewMemoryAnalyzer() *MemoryAnalyzer {
    return &MemoryAnalyzer{}
}

func (ma *MemoryAnalyzer) Sample() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    sample := MemorySample{
        Timestamp: time.Now(),
        Alloc:     m.Alloc,
        HeapAlloc: m.HeapAlloc,
        Sys:       m.Sys,
        NumGC:     m.NumGC,
    }

    ma.mutex.Lock()
    ma.samples = append(ma.samples, sample)

    // Keep only last 1000 samples
    if len(ma.samples) > 1000 {
        ma.samples = ma.samples[len(ma.samples)-1000:]
    }
    ma.mutex.Unlock()
}

func (ma *MemoryAnalyzer) GetTrend() (allocTrend, gcTrend float64) {
    ma.mutex.Lock()
    defer ma.mutex.Unlock()

    if len(ma.samples) < 2 {
        return 0, 0
    }

    first := ma.samples[0]
    last := ma.samples[len(ma.samples)-1]
    duration := last.Timestamp.Sub(first.Timestamp).Seconds()

    if duration > 0 {
        allocTrend = float64(int64(last.Alloc)-int64(first.Alloc)) / duration
        gcTrend = float64(last.NumGC-first.NumGC) / duration
    }

    return allocTrend, gcTrend
}
```

## âš¡ CPU Optimization

### Performance-Critical Code Patterns
```go
// internal/optimization/cpu.go - Generated by claude-flow
package optimization

import (
    "math/bits"
    "runtime"
    "sync"
    "sync/atomic"
    "unsafe"
)

// Lock-free counter
type LockFreeCounter struct {
    value int64
}

func (c *LockFreeCounter) Inc() int64 {
    return atomic.AddInt64(&c.value, 1)
}

func (c *LockFreeCounter) Dec() int64 {
    return atomic.AddInt64(&c.value, -1)
}

func (c *LockFreeCounter) Get() int64 {
    return atomic.LoadInt64(&c.value)
}

func (c *LockFreeCounter) Set(value int64) {
    atomic.StoreInt64(&c.value, value)
}

// High-performance map for read-heavy workloads
type FastMap struct {
    data   sync.Map
    length int64
}

func NewFastMap() *FastMap {
    return &FastMap{}
}

func (fm *FastMap) Store(key, value interface{}) {
    if _, loaded := fm.data.LoadOrStore(key, value); !loaded {
        atomic.AddInt64(&fm.length, 1)
    }
}

func (fm *FastMap) Load(key interface{}) (interface{}, bool) {
    return fm.data.Load(key)
}

func (fm *FastMap) Delete(key interface{}) {
    if _, loaded := fm.data.LoadAndDelete(key); loaded {
        atomic.AddInt64(&fm.length, -1)
    }
}

func (fm *FastMap) Len() int {
    return int(atomic.LoadInt64(&fm.length))
}

// CPU-optimized string operations
type StringOptimizer struct{}

// Fast string comparison using unsafe pointer comparison
func (so *StringOptimizer) FastEqual(a, b string) bool {
    if len(a) != len(b) {
        return false
    }

    if len(a) == 0 {
        return true
    }

    // For same-length strings, compare pointers first
    aPtr := (*[2]uintptr)(unsafe.Pointer(&a))
    bPtr := (*[2]uintptr)(unsafe.Pointer(&b))

    if aPtr[0] == bPtr[0] {
        return true // Same underlying array
    }

    return a == b // Fallback to standard comparison
}

// Fast hash function for strings
func (so *StringOptimizer) FastHash(s string) uint64 {
    if len(s) == 0 {
        return 0
    }

    var hash uint64 = 14695981039346656037 // FNV offset basis
    for i := 0; i < len(s); i++ {
        hash ^= uint64(s[i])
        hash *= 1099511628211 // FNV prime
    }
    return hash
}

// Bit manipulation utilities
type BitUtils struct{}

func (bu *BitUtils) IsPowerOfTwo(n uint64) bool {
    return n != 0 && (n&(n-1)) == 0
}

func (bu *BitUtils) NextPowerOfTwo(n uint64) uint64 {
    if n == 0 {
        return 1
    }
    return 1 << (64 - bits.LeadingZeros64(n-1))
}

func (bu *BitUtils) PopCount(n uint64) int {
    return bits.OnesCount64(n)
}

func (bu *BitUtils) TrailingZeros(n uint64) int {
    return bits.TrailingZeros64(n)
}

// SIMD-like operations for slices
type SliceProcessor struct{}

func (sp *SliceProcessor) SumInt64(slice []int64) int64 {
    var sum int64

    // Process in chunks for better cache locality
    const chunkSize = 4
    chunks := len(slice) / chunkSize

    for i := 0; i < chunks; i++ {
        base := i * chunkSize
        sum += slice[base] + slice[base+1] + slice[base+2] + slice[base+3]
    }

    // Handle remaining elements
    for i := chunks * chunkSize; i < len(slice); i++ {
        sum += slice[i]
    }

    return sum
}

func (sp *SliceProcessor) MaxInt64(slice []int64) int64 {
    if len(slice) == 0 {
        return 0
    }

    max := slice[0]
    for _, v := range slice[1:] {
        if v > max {
            max = v
        }
    }
    return max
}

// Parallel processing with work stealing
type WorkStealer struct {
    workers   int
    workQueue chan func()
    wg        sync.WaitGroup
}

func NewWorkStealer(workers int) *WorkStealer {
    if workers <= 0 {
        workers = runtime.NumCPU()
    }

    ws := &WorkStealer{
        workers:   workers,
        workQueue: make(chan func(), workers*2),
    }

    // Start workers
    for i := 0; i < workers; i++ {
        go ws.worker()
    }

    return ws
}

func (ws *WorkStealer) worker() {
    for work := range ws.workQueue {
        work()
        ws.wg.Done()
    }
}

func (ws *WorkStealer) Submit(work func()) {
    ws.wg.Add(1)
    ws.workQueue <- work
}

func (ws *WorkStealer) Wait() {
    ws.wg.Wait()
}

func (ws *WorkStealer) Close() {
    close(ws.workQueue)
}

// Cache-friendly data structures
type CacheFriendlyQueue struct {
    items []interface{}
    head  int
    tail  int
    size  int
    mask  int // size - 1 for power of 2 sizes
}

func NewCacheFriendlyQueue(size int) *CacheFriendlyQueue {
    // Round up to next power of 2
    bu := BitUtils{}
    actualSize := int(bu.NextPowerOfTwo(uint64(size)))

    return &CacheFriendlyQueue{
        items: make([]interface{}, actualSize),
        mask:  actualSize - 1,
    }
}

func (q *CacheFriendlyQueue) Enqueue(item interface{}) bool {
    if q.size >= len(q.items) {
        return false // Queue full
    }

    q.items[q.tail] = item
    q.tail = (q.tail + 1) & q.mask
    q.size++
    return true
}

func (q *CacheFriendlyQueue) Dequeue() (interface{}, bool) {
    if q.size == 0 {
        return nil, false // Queue empty
    }

    item := q.items[q.head]
    q.items[q.head] = nil // Help GC
    q.head = (q.head + 1) & q.mask
    q.size--
    return item, true
}

func (q *CacheFriendlyQueue) Len() int {
    return q.size
}

func (q *CacheFriendlyQueue) Cap() int {
    return len(q.items)
}
```

## ðŸ“ˆ Benchmarking and Analysis

### Comprehensive Benchmarking Suite
```bash
# Generate benchmarking suite with claude-flow
npx claude-flow sparc run perf-analyzer "create comprehensive benchmarking suite for Go application"
```

**Generated Benchmark Code:**
```go
// test/benchmark/performance_test.go - Generated by claude-flow
package benchmark

import (
    "crypto/rand"
    "testing"
    "time"

    "github.com/username/project/internal/optimization"
)

// Memory allocation benchmarks
func BenchmarkMemoryOperations(b *testing.B) {
    b.Run("SliceAppend", func(b *testing.B) {
        b.ReportAllocs()
        for i := 0; i < b.N; i++ {
            slice := make([]int, 0)
            for j := 0; j < 1000; j++ {
                slice = append(slice, j)
            }
        }
    })

    b.Run("SlicePrealloc", func(b *testing.B) {
        b.ReportAllocs()
        for i := 0; i < b.N; i++ {
            slice := make([]int, 0, 1000)
            for j := 0; j < 1000; j++ {
                slice = append(slice, j)
            }
        }
    })

    b.Run("ObjectPool", func(b *testing.B) {
        pool := optimization.NewPool(func() []int {
            return make([]int, 0, 1000)
        })

        b.ReportAllocs()
        b.ResetTimer()

        for i := 0; i < b.N; i++ {
            slice := pool.Get()
            for j := 0; j < 1000; j++ {
                slice = append(slice, j)
            }
            slice = slice[:0] // Reset length
            pool.Put(slice)
        }
    })
}

// String operation benchmarks
func BenchmarkStringOperations(b *testing.B) {
    optimizer := &optimization.StringOptimizer{}
    str1 := "benchmark_string_for_comparison"
    str2 := "benchmark_string_for_comparison"
    str3 := "different_string_for_comparison"

    b.Run("StandardEqual", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            _ = str1 == str2
            _ = str1 == str3
        }
    })

    b.Run("FastEqual", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            _ = optimizer.FastEqual(str1, str2)
            _ = optimizer.FastEqual(str1, str3)
        }
    })

    b.Run("StandardHash", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            _ = hash(str1)
        }
    })

    b.Run("FastHash", func(b *testing.B) {
        for i := 0; i < b.N; i++ {
            _ = optimizer.FastHash(str1)
        }
    })
}

// Concurrent operation benchmarks
func BenchmarkConcurrentOperations(b *testing.B) {
    b.Run("Mutex", func(b *testing.B) {
        var counter int64
        var mu sync.Mutex

        b.RunParallel(func(pb *testing.PB) {
            for pb.Next() {
                mu.Lock()
                counter++
                mu.Unlock()
            }
        })
    })

    b.Run("Atomic", func(b *testing.B) {
        var counter int64

        b.RunParallel(func(pb *testing.PB) {
            for pb.Next() {
                atomic.AddInt64(&counter, 1)
            }
        })
    })

    b.Run("LockFreeCounter", func(b *testing.B) {
        counter := &optimization.LockFreeCounter{}

        b.RunParallel(func(pb *testing.PB) {
            for pb.Next() {
                counter.Inc()
            }
        })
    })
}

// Data structure benchmarks
func BenchmarkDataStructures(b *testing.B) {
    b.Run("StandardMap", func(b *testing.B) {
        m := make(map[string]int)
        keys := generateKeys(1000)

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            key := keys[i%len(keys)]
            m[key] = i
            _ = m[key]
        }
    })

    b.Run("SyncMap", func(b *testing.B) {
        var m sync.Map
        keys := generateKeys(1000)

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            key := keys[i%len(keys)]
            m.Store(key, i)
            _, _ = m.Load(key)
        }
    })

    b.Run("FastMap", func(b *testing.B) {
        m := optimization.NewFastMap()
        keys := generateKeys(1000)

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            key := keys[i%len(keys)]
            m.Store(key, i)
            _, _ = m.Load(key)
        }
    })
}

// I/O operation benchmarks
func BenchmarkIOOperations(b *testing.B) {
    data := make([]byte, 1024)
    rand.Read(data)

    b.Run("JSONMarshal", func(b *testing.B) {
        payload := map[string]interface{}{
            "data":      string(data),
            "timestamp": time.Now(),
            "count":     1000,
        }

        b.ReportAllocs()
        for i := 0; i < b.N; i++ {
            _, _ = json.Marshal(payload)
        }
    })

    b.Run("PooledJSONMarshal", func(b *testing.B) {
        marshaler := optimization.NewJSONMarshaler()
        payload := map[string]interface{}{
            "data":      string(data),
            "timestamp": time.Now(),
            "count":     1000,
        }

        b.ReportAllocs()
        for i := 0; i < b.N; i++ {
            _, _ = marshaler.Marshal(payload)
        }
    })
}

// Memory access pattern benchmarks
func BenchmarkMemoryAccess(b *testing.B) {
    const size = 1024 * 1024

    b.Run("SequentialAccess", func(b *testing.B) {
        data := make([]int64, size)

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            var sum int64
            for j := 0; j < len(data); j++ {
                sum += data[j]
            }
            _ = sum
        }
    })

    b.Run("RandomAccess", func(b *testing.B) {
        data := make([]int64, size)
        indices := make([]int, size)
        for i := range indices {
            indices[i] = rand.Intn(size)
        }

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            var sum int64
            for _, idx := range indices {
                sum += data[idx]
            }
            _ = sum
        }
    })

    b.Run("StridedAccess", func(b *testing.B) {
        data := make([]int64, size)

        b.ResetTimer()
        for i := 0; i < b.N; i++ {
            var sum int64
            for j := 0; j < len(data); j += 16 { // Cache line stride
                sum += data[j]
            }
            _ = sum
        }
    })
}

// Helper functions
func generateKeys(count int) []string {
    keys := make([]string, count)
    for i := 0; i < count; i++ {
        keys[i] = fmt.Sprintf("key_%d", i)
    }
    return keys
}

func hash(s string) uint64 {
    h := fnv.New64a()
    h.Write([]byte(s))
    return h.Sum64()
}
```

## ðŸ”§ Performance Tuning Strategies

### Garbage Collection Optimization
```go
// internal/gc/optimization.go - Generated by claude-flow
package gc

import (
    "runtime"
    "runtime/debug"
    "time"
)

type GCOptimizer struct {
    targetGCPercent int
    maxHeapSize     int64
    monitoring      bool
}

func NewGCOptimizer() *GCOptimizer {
    return &GCOptimizer{
        targetGCPercent: 100,
        maxHeapSize:     0,
        monitoring:      false,
    }
}

// Optimize GC for low-latency applications
func (gc *GCOptimizer) OptimizeForLowLatency() {
    // Reduce GC target percentage for more frequent, smaller collections
    gc.targetGCPercent = 50
    debug.SetGCPercent(gc.targetGCPercent)

    // Set a lower memory limit to trigger GC sooner
    debug.SetMemoryLimit(1 << 30) // 1GB limit
}

// Optimize GC for high-throughput applications
func (gc *GCOptimizer) OptimizeForThroughput() {
    // Increase GC target percentage for less frequent collections
    gc.targetGCPercent = 200
    debug.SetGCPercent(gc.targetGCPercent)

    // Disable memory limit to allow larger heap
    debug.SetMemoryLimit(math.MaxInt64)
}

// Adaptive GC tuning based on metrics
func (gc *GCOptimizer) StartAdaptiveTuning(ctx context.Context) {
    if gc.monitoring {
        return
    }

    gc.monitoring = true

    go func() {
        ticker := time.NewTicker(30 * time.Second)
        defer ticker.Stop()

        var lastGCCount uint32
        var lastPauseTotal time.Duration

        for {
            select {
            case <-ticker.C:
                var m runtime.MemStats
                runtime.ReadMemStats(&m)

                // Calculate GC frequency and pause times
                gcCount := m.NumGC - lastGCCount
                pauseIncrease := time.Duration(m.PauseTotalNs) - lastPauseTotal

                lastGCCount = m.NumGC
                lastPauseTotal = time.Duration(m.PauseTotalNs)

                // Adjust GC percent based on pause times
                avgPause := pauseIncrease / time.Duration(gcCount)
                if avgPause > 10*time.Millisecond && gc.targetGCPercent > 50 {
                    gc.targetGCPercent -= 10
                    debug.SetGCPercent(gc.targetGCPercent)
                } else if avgPause < 1*time.Millisecond && gc.targetGCPercent < 200 {
                    gc.targetGCPercent += 10
                    debug.SetGCPercent(gc.targetGCPercent)
                }

            case <-ctx.Done():
                gc.monitoring = false
                return
            }
        }
    }()
}

// Force GC and return timing information
func (gc *GCOptimizer) ForceGCWithTiming() time.Duration {
    start := time.Now()
    runtime.GC()
    return time.Since(start)
}

// Get current GC statistics
func (gc *GCOptimizer) GetGCStats() GCStats {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    return GCStats{
        NumGC:        m.NumGC,
        PauseTotal:   time.Duration(m.PauseTotalNs),
        HeapAlloc:    m.HeapAlloc,
        HeapSys:      m.HeapSys,
        HeapIdle:     m.HeapIdle,
        HeapInuse:    m.HeapInuse,
        NextGC:       m.NextGC,
        LastGC:       time.Unix(0, int64(m.LastGC)),
        GCPercent:    gc.targetGCPercent,
    }
}

type GCStats struct {
    NumGC        uint32
    PauseTotal   time.Duration
    HeapAlloc    uint64
    HeapSys      uint64
    HeapIdle     uint64
    HeapInuse    uint64
    NextGC       uint64
    LastGC       time.Time
    GCPercent    int
}
```

## ðŸš€ Claude-Flow Performance Commands

```bash
# Comprehensive performance analysis
npx claude-flow sparc run perf-analyzer "analyze and optimize Go application performance"

# Specific optimizations
npx claude-flow sparc run perf-analyzer "optimize memory allocation patterns"
npx claude-flow sparc run perf-analyzer "reduce CPU bottlenecks in concurrent operations"
npx claude-flow sparc run perf-analyzer "optimize database query performance"

# Benchmarking
npx claude-flow sparc run tester "create comprehensive benchmark suite"
npx claude-flow sparc run perf-analyzer "profile application under load"

# Monitoring setup
npx claude-flow sparc run coder "add performance monitoring and alerting"
```

## ðŸ“Š Performance Monitoring Dashboard

```bash
# Generate monitoring dashboard with claude-flow
npx claude-flow sparc run coder "create Prometheus and Grafana dashboard for Go application performance"
```

## ðŸ›¡ï¸ Best Practices

### 1. Profiling Strategy
- **Profile in Production**: Use production-like data and load
- **Profile Regularly**: Make it part of your development cycle
- **Focus on Hotspots**: Optimize the 20% that matters
- **Measure Before/After**: Always validate optimizations

### 2. Memory Management
- **Use Object Pools**: For frequently allocated objects
- **Minimize Allocations**: Reuse buffers and slices
- **Avoid Memory Leaks**: Close resources and clear references
- **Monitor GC**: Tune garbage collection for your workload

### 3. CPU Optimization
- **Profile CPU Usage**: Identify computational bottlenecks
- **Use Efficient Algorithms**: Choose the right data structures
- **Minimize Lock Contention**: Use lock-free approaches when possible
- **Leverage Concurrency**: But avoid over-parallelization

### 4. I/O Performance
- **Connection Pooling**: Reuse database and HTTP connections
- **Buffered I/O**: Use appropriate buffer sizes
- **Async Operations**: Use goroutines for I/O-bound tasks
- **Cache Strategically**: Cache expensive computations and queries

**Next Steps:**
- [Microservices](../microservices/) - Build scalable distributed systems
- [Integration](../integration/) - Advanced claude-flow patterns
- [Examples](../examples/) - See optimized production applications