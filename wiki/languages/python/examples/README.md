# Python Examples with Claude-Flow

Practical examples and templates for Python development using claude-flow-novice orchestration. Each example demonstrates real-world applications with complete agent coordination workflows.

## ğŸš€ Quick Examples

### FastAPI Microservice

```bash
# Complete FastAPI microservice in one command
npx claude-flow@alpha sparc tdd "E-commerce API with payment processing"

# This automatically spawns coordinated agents:
# - Backend Developer: FastAPI application structure
# - Database Architect: SQLAlchemy models and migrations
# - API Designer: OpenAPI documentation and endpoints
# - Security Engineer: JWT authentication and authorization
# - Test Engineer: Comprehensive test suite
# - DevOps Engineer: Docker and deployment configuration
```

### Machine Learning Pipeline

```bash
# End-to-end ML pipeline
npx claude-flow@alpha sparc run architect "Customer churn prediction model"

# Coordinated ML team:
# - Data Scientist: EDA and feature engineering
# - ML Engineer: Model training and evaluation
# - Data Engineer: ETL pipeline and data validation
# - MLOps Engineer: Model deployment and monitoring
```

### Data Analysis Dashboard

```bash
# Interactive data dashboard
Task("Data Analyst", "Create Streamlit dashboard for sales analysis", "data-scientist")
Task("Visualization Expert", "Build interactive charts with Plotly", "coder")
Task("Backend Developer", "Create data API endpoints", "backend-dev")
```

## ğŸ“ Example Categories

### [Web Development Examples](./web-development/)
- **FastAPI REST API** - Modern async API with authentication
- **Django Web Application** - Full-stack web app with admin
- **Flask Microservice** - Lightweight API service
- **WebSocket Chat Application** - Real-time communication

### [Data Science Examples](./data-science/)
- **Jupyter Analysis Notebook** - Complete data analysis workflow
- **Machine Learning Pipeline** - End-to-end ML project
- **Time Series Forecasting** - ARIMA and Prophet models
- **Computer Vision Project** - Image classification with CNN

### [Testing Examples](./testing/)
- **Comprehensive Test Suite** - Unit, integration, and E2E tests
- **Performance Testing** - Load testing with Locust
- **Test Automation** - CI/CD testing pipeline
- **Mock and Fixture Patterns** - Advanced testing techniques

### [CLI Applications](./cli/)
- **Command Line Tool** - Click-based CLI application
- **Data Processing Script** - Batch data processing
- **System Administration Tool** - Server management utilities
- **Configuration Manager** - Environment setup automation

## ğŸ› ï¸ Complete Project Templates

### 1. E-commerce API (FastAPI + PostgreSQL)

```python
# Project structure generated by claude-flow-novice agents
ecommerce-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Settings and configuration
â”‚   â”‚   â”œâ”€â”€ database.py        # Database connection
â”‚   â”‚   â”œâ”€â”€ security.py        # Authentication and security
â”‚   â”‚   â””â”€â”€ exceptions.py      # Custom exceptions
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ deps.py            # API dependencies
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ auth.py        # Authentication endpoints
â”‚   â”‚       â”œâ”€â”€ users.py       # User management
â”‚   â”‚       â”œâ”€â”€ products.py    # Product catalog
â”‚   â”‚       â”œâ”€â”€ orders.py      # Order management
â”‚   â”‚       â””â”€â”€ payments.py    # Payment processing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user.py           # User model
â”‚   â”‚   â”œâ”€â”€ product.py        # Product models
â”‚   â”‚   â”œâ”€â”€ order.py          # Order models
â”‚   â”‚   â””â”€â”€ payment.py        # Payment models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user.py           # User Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ product.py        # Product schemas
â”‚   â”‚   â”œâ”€â”€ order.py          # Order schemas
â”‚   â”‚   â””â”€â”€ payment.py        # Payment schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user_service.py   # User business logic
â”‚   â”‚   â”œâ”€â”€ product_service.py # Product business logic
â”‚   â”‚   â”œâ”€â”€ order_service.py  # Order business logic
â”‚   â”‚   â”œâ”€â”€ payment_service.py # Payment processing
â”‚   â”‚   â””â”€â”€ email_service.py  # Email notifications
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ security.py       # Security utilities
â”‚       â””â”€â”€ validators.py     # Custom validators
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py          # Test configuration
â”‚   â”œâ”€â”€ unit/                # Unit tests
â”‚   â”œâ”€â”€ integration/         # Integration tests
â”‚   â””â”€â”€ e2e/                 # End-to-end tests
â”œâ”€â”€ alembic/                 # Database migrations
â”œâ”€â”€ docs/                    # API documentation
â”œâ”€â”€ docker-compose.yml       # Development environment
â”œâ”€â”€ Dockerfile              # Production container
â”œâ”€â”€ pyproject.toml          # Project configuration
â””â”€â”€ README.md               # Project documentation
```

#### Key Features Generated:
- **Authentication**: JWT-based auth with refresh tokens
- **User Management**: Registration, profile management, roles
- **Product Catalog**: Categories, products, inventory management
- **Order Processing**: Cart, checkout, order tracking
- **Payment Integration**: Stripe integration, webhook handling
- **Email Notifications**: Order confirmations, password reset
- **API Documentation**: Automatic OpenAPI/Swagger docs
- **Testing**: 90%+ test coverage with pytest
- **Deployment**: Docker containerization, CI/CD pipeline

### 2. Data Science Platform (Jupyter + MLflow + Streamlit)

```python
# AI-generated data science platform
data-science-platform/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb    # EDA and data profiling
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb # Feature creation
â”‚   â”œâ”€â”€ 03_model_training.ipynb      # Model development
â”‚   â”œâ”€â”€ 04_model_evaluation.ipynb    # Performance analysis
â”‚   â””â”€â”€ 05_model_deployment.ipynb    # Deployment preparation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py               # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ preprocessor.py         # Data preprocessing
â”‚   â”‚   â””â”€â”€ validator.py            # Data validation
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engineering.py          # Feature engineering
â”‚   â”‚   â””â”€â”€ selection.py            # Feature selection
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ training.py             # Model training
â”‚   â”‚   â”œâ”€â”€ evaluation.py           # Model evaluation
â”‚   â”‚   â””â”€â”€ prediction.py           # Inference pipeline
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ plots.py               # Plotting utilities
â”‚   â”‚   â””â”€â”€ dashboard.py           # Dashboard components
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # Configuration management
â”‚       â””â”€â”€ logger.py              # Logging setup
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                     # Streamlit dashboard
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ data_exploration.py    # Data exploration page
â”‚   â”‚   â”œâ”€â”€ model_training.py      # Model training interface
â”‚   â”‚   â””â”€â”€ predictions.py         # Prediction interface
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ metrics.py             # Metrics components
â”‚       â””â”€â”€ visualizations.py      # Chart components
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                    # FastAPI prediction service
â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”œâ”€â”€ predict.py             # Prediction endpoints
â”‚   â”‚   â””â”€â”€ model_management.py    # Model management
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ schemas.py             # API schemas
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                      # Unit tests
â”‚   â”œâ”€â”€ integration/               # Integration tests
â”‚   â””â”€â”€ data/                      # Test data
â”œâ”€â”€ mlruns/                        # MLflow experiment tracking
â”œâ”€â”€ models/                        # Trained models
â”œâ”€â”€ data/                          # Dataset storage
â”‚   â”œâ”€â”€ raw/                       # Raw data
â”‚   â”œâ”€â”€ processed/                 # Processed data
â”‚   â””â”€â”€ external/                  # External data sources
â”œâ”€â”€ docker-compose.yml             # Multi-service setup
â””â”€â”€ requirements.txt               # Dependencies
```

#### Platform Features:
- **Data Pipeline**: Automated ETL with validation
- **Experiment Tracking**: MLflow integration
- **Model Management**: Versioning and deployment
- **Interactive Dashboard**: Streamlit-based UI
- **Prediction API**: RESTful prediction service
- **Monitoring**: Model performance tracking
- **Collaboration**: Shared notebooks and experiments

### 3. CLI Data Processing Tool

```python
# Advanced CLI tool generated by agents
data-processor-cli/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py               # Main CLI interface
â”‚   â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ process.py        # Data processing commands
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py        # Analysis commands
â”‚   â”‚   â”‚   â”œâ”€â”€ transform.py      # Transformation commands
â”‚   â”‚   â”‚   â””â”€â”€ export.py         # Export commands
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ progress.py       # Progress bars
â”‚   â”‚       â””â”€â”€ validation.py     # Input validation
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ csv_processor.py      # CSV processing
â”‚   â”‚   â”œâ”€â”€ json_processor.py     # JSON processing
â”‚   â”‚   â”œâ”€â”€ excel_processor.py    # Excel processing
â”‚   â”‚   â””â”€â”€ database_processor.py # Database operations
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ statistical.py       # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ quality.py           # Data quality checks
â”‚   â”‚   â””â”€â”€ profiling.py         # Data profiling
â”‚   â””â”€â”€ transformers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cleaning.py          # Data cleaning
â”‚       â”œâ”€â”€ aggregation.py       # Data aggregation
â”‚       â””â”€â”€ normalization.py     # Data normalization
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ setup.py                     # Package setup
â””â”€â”€ README.md
```

#### CLI Features:
- **Multi-format Support**: CSV, JSON, Excel, databases
- **Data Analysis**: Statistical analysis, profiling
- **Data Quality**: Validation, cleaning, anomaly detection
- **Transformations**: Filtering, aggregation, normalization
- **Progress Tracking**: Real-time progress bars
- **Configuration**: YAML-based configuration files
- **Extensibility**: Plugin architecture

## ğŸ¯ Agent Coordination Examples

### Full-Stack Development Team

```bash
# Initialize hierarchical swarm for full-stack development
npx claude-flow@alpha mcp swarm_init --topology hierarchical --maxAgents 8

# Spawn coordinated development team
Task("Project Architect", "Design system architecture and technology stack", "system-architect")
Task("Backend Developer", "Implement REST API with FastAPI and PostgreSQL", "backend-dev")
Task("Frontend Developer", "Create React frontend with TypeScript", "coder")
Task("Database Engineer", "Design database schema and optimization", "code-analyzer")
Task("DevOps Engineer", "Setup CI/CD pipeline and deployment", "cicd-engineer")
Task("Security Auditor", "Implement security best practices", "reviewer")
Task("Test Engineer", "Create comprehensive test suite", "tester")
Task("Documentation Writer", "Generate API docs and user guides", "api-docs")

# Coordination hooks for seamless collaboration
# Pre-task: Each agent checks shared memory for project context
# During: Agents update memory with progress and decisions
# Post-task: Integration testing and code review
```

### Data Science Research Team

```bash
# Research-focused swarm topology
npx claude-flow@alpha mcp swarm_init --topology mesh --maxAgents 6

# Spawn research and analysis team
Task("Research Coordinator", "Literature review and research planning", "researcher")
Task("Data Engineer", "Build robust data pipelines", "data-engineer")
Task("Data Scientist", "Exploratory analysis and hypothesis testing", "data-scientist")
Task("ML Engineer", "Model development and hyperparameter tuning", "ml-developer")
Task("Statistician", "Statistical validation and significance testing", "researcher")
Task("Visualization Expert", "Create publication-ready visualizations", "coder")

# Memory coordination for research insights
npx claude-flow@alpha mcp memory_usage --action store --key "research/hypothesis" --value "Customer churn correlates with support ticket frequency"
npx claude-flow@alpha mcp memory_usage --action store --key "research/methodology" --value "A/B testing with statistical significance p<0.05"
```

### Performance Optimization Team

```bash
# Specialized performance optimization swarm
Task("Performance Analyst", "Profile application and identify bottlenecks", "performance-optimizer")
Task("Database Optimizer", "Optimize queries and database performance", "code-analyzer")
Task("Caching Specialist", "Implement Redis caching strategies", "backend-dev")
Task("Load Tester", "Create comprehensive load testing suite", "tester")
Task("Monitoring Engineer", "Setup APM and performance monitoring", "cicd-engineer")

# Performance benchmarking and reporting
npx claude-flow@alpha mcp benchmark_run --type python --components "api,database,cache"
npx claude-flow@alpha mcp performance_report --format detailed --timeframe 24h
```

## ğŸ”„ Workflow Automation Examples

### Automated Code Review Workflow

```python
# scripts/automated_review.py - AI-powered code review
import subprocess
import json
from pathlib import Path

class AutomatedCodeReview:
    """Automated code review with claude-flow-novice agents."""

    def __init__(self):
        self.review_checklist = [
            "Code quality and style",
            "Security vulnerabilities",
            "Performance issues",
            "Test coverage",
            "Documentation completeness"
        ]

    def trigger_review(self, pr_number=None):
        """Trigger automated code review."""
        if pr_number:
            # Review specific PR
            subprocess.run([
                "npx", "claude-flow@alpha", "mcp", "github_pr_manage",
                "--action", "review",
                "--pr", str(pr_number)
            ])
        else:
            # Review current changes
            self.review_current_changes()

    def review_current_changes(self):
        """Review current git changes."""
        # Get changed files
        result = subprocess.run([
            "git", "diff", "--name-only", "HEAD~1"
        ], capture_output=True, text=True)

        changed_files = result.stdout.strip().split('\n')
        python_files = [f for f in changed_files if f.endswith('.py')]

        if python_files:
            # Spawn review agents for Python files
            for file in python_files:
                subprocess.run([
                    "npx", "claude-flow@alpha", "task",
                    "Code Reviewer",
                    f"Review {file} for quality, security, and performance",
                    "reviewer"
                ])

    def generate_review_report(self):
        """Generate comprehensive review report."""
        # Run static analysis tools
        analysis_results = {}

        # Flake8 analysis
        flake8_result = subprocess.run([
            "flake8", ".", "--format=json"
        ], capture_output=True, text=True)

        if flake8_result.stdout:
            analysis_results['linting'] = json.loads(flake8_result.stdout)

        # Security analysis with bandit
        bandit_result = subprocess.run([
            "bandit", "-r", ".", "-f", "json"
        ], capture_output=True, text=True)

        if bandit_result.stdout:
            analysis_results['security'] = json.loads(bandit_result.stdout)

        # Generate report
        report = self.create_review_report(analysis_results)

        # Save and notify
        with open('reports/code_review.md', 'w') as f:
            f.write(report)

        subprocess.run([
            "npx", "claude-flow@alpha", "hooks", "notify",
            "--message", "Automated code review completed"
        ])

        return report

    def create_review_report(self, analysis_results):
        """Create formatted review report."""
        report = """
# Automated Code Review Report

## Summary
Automated analysis completed with the following findings:

"""

        if 'linting' in analysis_results:
            linting_issues = len(analysis_results['linting'])
            report += f"- **Linting Issues**: {linting_issues} issues found\n"

        if 'security' in analysis_results:
            security_issues = len(analysis_results['security'].get('results', []))
            report += f"- **Security Issues**: {security_issues} issues found\n"

        report += "\n## Detailed Findings\n\n"

        # Add detailed findings
        if 'linting' in analysis_results:
            report += "### Code Quality Issues\n"
            for issue in analysis_results['linting'][:10]:  # Top 10 issues
                report += f"- {issue.get('filename', 'Unknown')}: {issue.get('text', 'Issue')}\n"

        if 'security' in analysis_results:
            report += "\n### Security Issues\n"
            for issue in analysis_results['security'].get('results', [])[:5]:  # Top 5 security issues
                report += f"- **{issue.get('test_name', 'Security Issue')}**: {issue.get('issue_text', 'Description')}\n"

        return report

# Usage in CI/CD pipeline
if __name__ == "__main__":
    import sys

    reviewer = AutomatedCodeReview()

    if len(sys.argv) > 1:
        pr_number = int(sys.argv[1])
        reviewer.trigger_review(pr_number)
    else:
        reviewer.review_current_changes()

    reviewer.generate_review_report()
```

### Continuous Training Pipeline

```python
# scripts/ml_pipeline.py - Automated ML pipeline
import mlflow
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import subprocess
import schedule
import time

class MLPipeline:
    """Automated ML training and deployment pipeline."""

    def __init__(self):
        mlflow.set_experiment("automated-training")
        self.model_performance_threshold = 0.85

    def run_daily_training(self):
        """Run daily model training and evaluation."""
        try:
            # Notify start
            subprocess.run([
                "npx", "claude-flow@alpha", "hooks", "notify",
                "--message", "Daily ML training started"
            ])

            # Load fresh data
            data = self.load_training_data()

            # Train model
            model, metrics = self.train_model(data)

            # Evaluate performance
            if metrics['accuracy'] > self.model_performance_threshold:
                self.deploy_model(model, metrics)
            else:
                self.trigger_investigation(metrics)

        except Exception as e:
            self.handle_pipeline_error(e)

    def load_training_data(self):
        """Load and preprocess training data."""
        # This would connect to your data sources
        # For example: database, S3, APIs, etc.

        # Simulate data loading
        np.random.seed(42)
        n_samples = 10000
        n_features = 20

        X = np.random.randn(n_samples, n_features)
        y = (X[:, 0] + X[:, 1] + np.random.randn(n_samples) * 0.1) > 0

        return pd.DataFrame(X), pd.Series(y)

    def train_model(self, data):
        """Train ML model with tracking."""
        X, y = data

        with mlflow.start_run():
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # Train model
            model = RandomForestClassifier(
                n_estimators=100,
                random_state=42
            )
            model.fit(X_train, y_train)

            # Evaluate
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

            # Log metrics
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_param("n_estimators", 100)

            # Log model
            mlflow.sklearn.log_model(model, "model")

            metrics = {
                'accuracy': accuracy,
                'classification_report': classification_report(y_test, y_pred, output_dict=True)
            }

            return model, metrics

    def deploy_model(self, model, metrics):
        """Deploy model if performance is acceptable."""
        # Register model in MLflow
        model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
        model_version = mlflow.register_model(model_uri, "production-model")

        # Transition to production
        client = mlflow.tracking.MlflowClient()
        client.transition_model_version_stage(
            name="production-model",
            version=model_version.version,
            stage="Production"
        )

        # Notify deployment
        subprocess.run([
            "npx", "claude-flow@alpha", "hooks", "notify",
            "--message", f"Model deployed with accuracy: {metrics['accuracy']:.3f}"
        ])

        # Update deployment configuration
        self.update_deployment_config(model_version.version, metrics)

    def trigger_investigation(self, metrics):
        """Trigger investigation when model performance drops."""
        # Spawn investigation agents
        subprocess.run([
            "npx", "claude-flow@alpha", "task",
            "Data Scientist",
            f"Investigate model performance drop. Accuracy: {metrics['accuracy']:.3f}",
            "data-scientist"
        ])

        subprocess.run([
            "npx", "claude-flow@alpha", "task",
            "ML Engineer",
            "Analyze feature drift and data quality issues",
            "ml-developer"
        ])

        # Store investigation context
        subprocess.run([
            "npx", "claude-flow@alpha", "mcp", "memory_usage",
            "--action", "store",
            "--key", "ml/investigation",
            "--value", f"Performance drop: {metrics['accuracy']:.3f} < {self.model_performance_threshold}"
        ])

    def handle_pipeline_error(self, error):
        """Handle pipeline errors gracefully."""
        error_message = f"ML Pipeline Error: {str(error)}"

        # Log error
        with open('logs/pipeline_errors.log', 'a') as f:
            f.write(f"{time.time()}: {error_message}\n")

        # Notify error
        subprocess.run([
            "npx", "claude-flow@alpha", "hooks", "notify",
            "--message", error_message
        ])

        # Trigger recovery workflow
        subprocess.run([
            "npx", "claude-flow@alpha", "task",
            "DevOps Engineer",
            f"Investigate and fix ML pipeline error: {error_message}",
            "cicd-engineer"
        ])

    def update_deployment_config(self, version, metrics):
        """Update deployment configuration."""
        config = {
            'model_version': version,
            'performance_metrics': metrics,
            'deployment_timestamp': time.time(),
            'status': 'active'
        }

        # Save configuration
        import json
        with open('config/model_deployment.json', 'w') as f:
            json.dump(config, f, indent=2)

# Scheduling setup
def setup_scheduling():
    """Setup automated scheduling."""
    pipeline = MLPipeline()

    # Schedule daily training
    schedule.every().day.at("02:00").do(pipeline.run_daily_training)

    # Schedule weekly model evaluation
    schedule.every().week.do(lambda: subprocess.run([
        "npx", "claude-flow@alpha", "task",
        "ML Engineer",
        "Comprehensive weekly model evaluation and performance analysis",
        "ml-developer"
    ]))

    return pipeline

if __name__ == "__main__":
    pipeline = setup_scheduling()

    # Run scheduler
    while True:
        schedule.run_pending()
        time.sleep(60)  # Check every minute
```

## ğŸ“š Documentation Examples

### Auto-Generated API Documentation

```python
# docs/api_doc_generator.py - Automated API documentation
from fastapi import FastAPI
from fastapi.openapi.utils import get_openapi
import json
import subprocess

class APIDocumentationGenerator:
    """Generate comprehensive API documentation."""

    def __init__(self, app: FastAPI):
        self.app = app

    def generate_openapi_spec(self):
        """Generate OpenAPI specification."""
        openapi_schema = get_openapi(
            title="Claude-Flow API",
            version="1.0.0",
            description="AI-generated API with comprehensive documentation",
            routes=self.app.routes,
        )

        # Save OpenAPI spec
        with open('docs/openapi.json', 'w') as f:
            json.dump(openapi_schema, f, indent=2)

        return openapi_schema

    def generate_markdown_docs(self):
        """Generate markdown documentation."""
        # Use claude-flow-novice to generate comprehensive docs
        subprocess.run([
            "npx", "claude-flow@alpha", "task",
            "Documentation Writer",
            "Generate comprehensive API documentation from OpenAPI spec",
            "api-docs"
        ])

    def generate_code_examples(self):
        """Generate code examples for different languages."""
        examples = {
            'python': self.generate_python_examples(),
            'javascript': self.generate_javascript_examples(),
            'curl': self.generate_curl_examples()
        }

        # Save examples
        for lang, code in examples.items():
            with open(f'docs/examples/{lang}.md', 'w') as f:
                f.write(code)

        return examples

    def generate_python_examples(self):
        """Generate Python client examples."""
        return """
# Python API Client Examples

```python
import requests

# Authentication
auth_response = requests.post(
    "https://api.example.com/auth/login",
    data={
        "username": "user@example.com",
        "password": "password"
    }
)
token = auth_response.json()["access_token"]

headers = {"Authorization": f"Bearer {token}"}

# Create user
user_data = {
    "email": "newuser@example.com",
    "full_name": "New User",
    "password": "securepassword"
}

response = requests.post(
    "https://api.example.com/users/",
    json=user_data,
    headers=headers
)

print(response.json())

# Get products
products = requests.get(
    "https://api.example.com/products/",
    headers=headers
)

print(products.json())
```
"""

    def generate_javascript_examples(self):
        """Generate JavaScript client examples."""
        return """
# JavaScript API Client Examples

```javascript
// Authentication
const authResponse = await fetch('https://api.example.com/auth/login', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/x-www-form-urlencoded',
    },
    body: new URLSearchParams({
        username: 'user@example.com',
        password: 'password'
    })
});

const { access_token } = await authResponse.json();

// Create user
const userData = {
    email: 'newuser@example.com',
    full_name: 'New User',
    password: 'securepassword'
};

const userResponse = await fetch('https://api.example.com/users/', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${access_token}`
    },
    body: JSON.stringify(userData)
});

const user = await userResponse.json();
console.log(user);

// Get products
const productsResponse = await fetch('https://api.example.com/products/', {
    headers: {
        'Authorization': `Bearer ${access_token}`
    }
});

const products = await productsResponse.json();
console.log(products);
```
"""

    def generate_curl_examples(self):
        """Generate cURL examples."""
        return """
# cURL API Examples

## Authentication
```bash
curl -X POST "https://api.example.com/auth/login" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "username=user@example.com&password=password"
```

## Create User
```bash
curl -X POST "https://api.example.com/users/" \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer YOUR_TOKEN" \
     -d '{
       "email": "newuser@example.com",
       "full_name": "New User",
       "password": "securepassword"
     }'
```

## Get Products
```bash
curl -X GET "https://api.example.com/products/" \
     -H "Authorization: Bearer YOUR_TOKEN"
```

## Search Products
```bash
curl -X GET "https://api.example.com/products/?search=laptop&category_id=1" \
     -H "Authorization: Bearer YOUR_TOKEN"
```
"""

# Usage
if __name__ == "__main__":
    from app.main import app

    doc_generator = APIDocumentationGenerator(app)
    doc_generator.generate_openapi_spec()
    doc_generator.generate_markdown_docs()
    doc_generator.generate_code_examples()
```

## ğŸš€ Getting Started with Examples

### 1. Choose Your Project Type
```bash
# Web API development
npx claude-flow@alpha init --template fastapi-microservice

# Data science project
npx claude-flow@alpha init --template ml-pipeline

# CLI application
npx claude-flow@alpha init --template cli-tool

# Full-stack application
npx claude-flow@alpha init --template fullstack-app
```

### 2. Customize with Agents
```bash
# Spawn specialized agents based on your needs
Task("Requirements Analyst", "Analyze project requirements and create specifications", "researcher")
Task("System Architect", "Design system architecture and technology choices", "system-architect")
Task("Lead Developer", "Coordinate development and ensure code quality", "coder")
```

### 3. Continuous Development
```bash
# Enable automated workflows
npx claude-flow@alpha hooks enable --project-type python
npx claude-flow@alpha automation setup --rules "test-on-commit,deploy-on-merge"
```

## ğŸ“ Contributing Examples

Want to contribute your own examples? Follow this pattern:

1. **Create project structure** using claude-flow-novice agents
2. **Document agent coordination** workflows
3. **Include comprehensive tests** and documentation
4. **Add memory integration** for context sharing
5. **Submit with performance metrics** and benchmarks

---

**Ready to build with Python and Claude-Flow?** Start with a [template project](./templates/) or explore [advanced workflows](./workflows/).