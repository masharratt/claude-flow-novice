name: Swarm E2E Test Pipeline with Dynamic Generation

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 4 * * *'  # Daily at 4 AM UTC
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'standard'
        type: choice
        options:
          - standard
          - regression
          - performance
          - swarm-only
          - full-suite
      generate_tests:
        description: 'Generate new tests from swarm activities'
        required: false
        default: true
        type: boolean
      cleanup_data:
        description: 'Clean up test data after execution'
        required: false
        default: true
        type: boolean

env:
  NODE_VERSION: '20'
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'
  TEST_PARALLELISM: '4'
  SWARM_TIMEOUT: '300'

jobs:
  setup-and-analyze:
    name: Setup and Analyze Swarm Changes
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.generate-matrix.outputs.matrix }}
      should-run-swarm: ${{ steps.changes.outputs.swarm-changes }}
      test-generation-needed: ${{ steps.changes.outputs.test-generation-needed }}
      affected-components: ${{ steps.analyze.outputs.components }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changes requiring swarm testing
        id: changes
        uses: dorny/paths-filter@v2
        with:
          filters: |
            swarm-changes:
              - 'src/swarm/**'
              - 'src/coordination/**'
              - 'src/agents/**'
              - 'src/hooks/**'
              - 'tests/swarm/**'
              - '.claude-flow/**'
            test-generation-needed:
              - 'src/swarm/**'
              - 'src/agents/**'
              - 'planning/**'
              - 'examples/**'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Analyze affected components
        id: analyze
        run: |
          node scripts/analyze-changes.js > analysis.json
          echo "components=$(cat analysis.json | jq -c '.affected_components')" >> $GITHUB_OUTPUT
          cat analysis.json

      - name: Generate dynamic test matrix
        id: generate-matrix
        run: |
          node scripts/generate-test-matrix.js \
            --mode="${{ github.event.inputs.test_mode || 'standard' }}" \
            --changes="${{ steps.changes.outputs }}" \
            --components="${{ steps.analyze.outputs.components }}" > matrix.json
          echo "matrix=$(cat matrix.json)" >> $GITHUB_OUTPUT
          cat matrix.json

  generate-tests:
    name: Generate Tests from Swarm Activities
    runs-on: ubuntu-latest
    needs: setup-and-analyze
    if: needs.setup-and-analyze.outputs.test-generation-needed == 'true' || github.event.inputs.generate_tests == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Initialize swarm for test generation
        run: |
          npx claude-flow@alpha swarm init --topology mesh --agents 3
          npx claude-flow@alpha hooks pre-task --description "Generate E2E tests from swarm patterns"

      - name: Generate E2E tests from swarm activities
        run: |
          node scripts/generate-swarm-tests.js \
            --components="${{ needs.setup-and-analyze.outputs.affected-components }}" \
            --output="tests/generated" \
            --format="playwright"

      - name: Generate performance tests
        run: |
          node scripts/generate-performance-tests.js \
            --swarm-configs="src/swarm/configs" \
            --output="tests/generated/performance"

      - name: Generate integration tests
        run: |
          node scripts/generate-integration-tests.js \
            --hooks="src/hooks" \
            --output="tests/generated/integration"

      - name: Validate generated tests
        run: |
          npx eslint tests/generated --fix
          node scripts/validate-generated-tests.js tests/generated

      - name: Upload generated tests
        uses: actions/upload-artifact@v3
        with:
          name: generated-tests
          path: tests/generated/
          retention-days: 1

  unit-tests:
    name: Unit Tests with Coverage
    runs-on: ubuntu-latest
    needs: setup-and-analyze

    strategy:
      matrix:
        node-version: [18, 20]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests with coverage
        run: |
          npm run test:coverage:unit
          npm run test:coverage:integration

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info
          flags: unit-tests,node-${{ matrix.node-version }}
          name: codecov-unit-${{ matrix.node-version }}

  swarm-coordination-tests:
    name: Swarm Coordination Tests (${{ matrix.topology }})
    runs-on: ubuntu-latest
    needs: [setup-and-analyze, generate-tests]
    if: always() && (needs.setup-and-analyze.outputs.should-run-swarm == 'true' || github.event.inputs.test_mode != 'standard')

    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup-and-analyze.outputs.test-matrix) }}

    timeout-minutes: 45

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Download generated tests
        uses: actions/download-artifact@v3
        with:
          name: generated-tests
          path: tests/generated/
        continue-on-error: true

      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('package-lock.json') }}

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Setup test environment
        run: |
          npm run db:setup:test
          mkdir -p test-results/swarm-logs
          mkdir -p test-data

      - name: Initialize swarm topology
        run: |
          npx claude-flow@alpha swarm init \
            --topology ${{ matrix.topology }} \
            --agents ${{ matrix.agents }} \
            --strategy ${{ matrix.strategy }}
          npx claude-flow@alpha hooks pre-task --description "E2E swarm testing for ${{ matrix.topology }}"

      - name: Start application with swarm coordination
        run: |
          npm run start:test:swarm &
          sleep 30
          curl --retry 10 --retry-delay 5 --retry-connrefused http://localhost:3000/health

      - name: Run swarm coordination tests
        run: |
          export SWARM_TOPOLOGY=${{ matrix.topology }}
          export SWARM_AGENTS=${{ matrix.agents }}
          export SWARM_STRATEGY=${{ matrix.strategy }}

          if [ -d "tests/generated" ]; then
            npx playwright test tests/generated/swarm/ --project=swarm-${{ matrix.topology }}
          fi

          npx playwright test tests/swarm/coordination/ --project=swarm-${{ matrix.topology }}
        timeout-minutes: 30
        env:
          SWARM_LOG_LEVEL: 'debug'
          COORDINATION_TIMEOUT: ${{ env.SWARM_TIMEOUT }}

      - name: Capture swarm metrics
        if: always()
        run: |
          npx claude-flow@alpha hooks post-task --task-id "swarm-${{ matrix.topology }}-test"
          npx claude-flow@alpha hooks session-end --export-metrics true
          cp -r .claude-flow/metrics test-results/swarm-logs/

      - name: Upload swarm test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: swarm-test-results-${{ matrix.topology }}
          path: |
            test-results/
            test-results/swarm-logs/
            playwright-report/
          retention-days: 7

  browser-tests:
    name: Browser Tests with Chrome MCP (${{ matrix.browser }})
    runs-on: ubuntu-latest
    needs: [setup-and-analyze, generate-tests]
    if: always()

    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]
        include:
          - browser: chromium
            use-chrome-mcp: true
          - browser: firefox
            use-chrome-mcp: false
          - browser: webkit
            use-chrome-mcp: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Download generated tests
        uses: actions/download-artifact@v3
        with:
          name: generated-tests
          path: tests/generated/
        continue-on-error: true

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Setup Chrome MCP integration
        if: matrix.use-chrome-mcp == true
        run: |
          # Install Chrome MCP server
          npm install @modelcontextprotocol/server-chrome

          # Configure Chrome MCP for testing
          mkdir -p .mcp-config
          cat > .mcp-config/chrome.json << 'EOF'
          {
            "mcpServers": {
              "chrome": {
                "command": "npx",
                "args": ["@modelcontextprotocol/server-chrome"],
                "env": {
                  "CHROME_HEADLESS": "true",
                  "CHROME_ARGS": "--no-sandbox,--disable-dev-shm-usage"
                }
              }
            }
          }
          EOF

      - name: Start application
        run: |
          npm run start:test &
          sleep 20

      - name: Run browser tests
        run: |
          if [ "${{ matrix.use-chrome-mcp }}" = "true" ]; then
            export CHROME_MCP_ENABLED=true
            export MCP_CONFIG_PATH=.mcp-config/chrome.json
          fi

          if [ -d "tests/generated/browser" ]; then
            npx playwright test tests/generated/browser/ --project=${{ matrix.browser }}
          fi

          npx playwright test tests/e2e/browser/ --project=${{ matrix.browser }}
        env:
          BROWSER: ${{ matrix.browser }}
          HEADLESS: true

      - name: Upload browser test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: browser-test-results-${{ matrix.browser }}
          path: |
            test-results/
            playwright-report/
          retention-days: 7

  performance-tests:
    name: Performance and Load Tests
    runs-on: ubuntu-latest
    needs: [setup-and-analyze, generate-tests]
    if: github.event.inputs.test_mode == 'performance' || github.event.inputs.test_mode == 'full-suite' || github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Download generated tests
        uses: actions/download-artifact@v3
        with:
          name: generated-tests
          path: tests/generated/
        continue-on-error: true

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Setup performance test environment
        run: |
          npm run db:setup:test
          npm run start:test:performance &
          sleep 30

      - name: Run performance benchmarks
        run: |
          if [ -d "tests/generated/performance" ]; then
            npx playwright test tests/generated/performance/
          fi

          npm run test:performance
          npx playwright test tests/performance/ --project=performance
        env:
          PERFORMANCE_MODE: 'benchmark'
          LOAD_TEST_DURATION: '300'

      - name: Generate performance report
        run: |
          node scripts/generate-performance-report.js \
            --input test-results/performance/ \
            --output test-results/performance-report.html \
            --format html

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            test-results/performance/
            test-results/performance-report.html
          retention-days: 30

  regression-tests:
    name: Regression Testing Suite
    runs-on: ubuntu-latest
    needs: [setup-and-analyze, unit-tests, swarm-coordination-tests]
    if: github.event.inputs.test_mode == 'regression' || github.event.inputs.test_mode == 'full-suite' || github.event_name == 'schedule'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps

      - name: Download baseline test data
        run: |
          curl -o test-data/regression-baselines.json \
            https://api.github.com/repos/${{ github.repository }}/releases/latest/assets/regression-baselines.json
        continue-on-error: true

      - name: Run regression test suite
        run: |
          npm run test:regression
          npx playwright test tests/regression/ --project=regression
        env:
          REGRESSION_MODE: 'full'
          BASELINE_PATH: 'test-data/regression-baselines.json'

      - name: Compare against baselines
        run: |
          node scripts/compare-regression-results.js \
            --current test-results/regression/ \
            --baseline test-data/regression-baselines.json \
            --output test-results/regression-comparison.json

      - name: Upload regression results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: regression-test-results
          path: |
            test-results/regression/
            test-results/regression-comparison.json
          retention-days: 30

  test-data-cleanup:
    name: Test Data Management and Cleanup
    runs-on: ubuntu-latest
    needs: [swarm-coordination-tests, browser-tests, performance-tests, regression-tests]
    if: always() && github.event.inputs.cleanup_data != 'false'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download all test artifacts
        uses: actions/download-artifact@v3

      - name: Clean up test databases
        run: |
          node scripts/cleanup-test-databases.js
          rm -rf test-data/temp-*
          rm -rf .claude-flow/test-sessions

      - name: Archive important test data
        run: |
          mkdir -p test-archive
          tar -czf test-archive/test-session-$(date +%Y%m%d-%H%M%S).tar.gz \
            test-results/ \
            .claude-flow/metrics/

      - name: Upload archived test data
        uses: actions/upload-artifact@v3
        with:
          name: test-archive
          path: test-archive/
          retention-days: 90

  generate-comprehensive-report:
    name: Generate Comprehensive Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, swarm-coordination-tests, browser-tests, performance-tests, regression-tests]
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download all test results
        uses: actions/download-artifact@v3

      - name: Generate comprehensive report
        run: |
          node scripts/generate-comprehensive-report.js \
            --input . \
            --output comprehensive-test-report.html \
            --format html \
            --include-metrics \
            --include-coverage \
            --include-performance

      - name: Generate test summary for PR
        if: github.event_name == 'pull_request'
        run: |
          node scripts/generate-pr-summary.js \
            --input . \
            --output pr-test-summary.md \
            --format markdown

      - name: Comment PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('pr-test-summary.md')) {
              const summary = fs.readFileSync('pr-test-summary.md', 'utf8');
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            }

      - name: Upload comprehensive report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: |
            comprehensive-test-report.html
            pr-test-summary.md
          retention-days: 30

  failure-notification:
    name: Handle Test Failures
    runs-on: ubuntu-latest
    needs: [unit-tests, swarm-coordination-tests, browser-tests, performance-tests, regression-tests]
    if: failure() && (github.event_name == 'push' || github.event_name == 'schedule')

    steps:
      - name: Create failure issue
        uses: actions/github-script@v6
        with:
          script: |
            const title = `Test Pipeline Failure - ${context.workflow} on ${context.ref}`;
            const body = `
            # Test Pipeline Failure Report

            **Workflow**: ${context.workflow}
            **Branch**: ${context.ref}
            **Commit**: ${context.sha}
            **Run ID**: ${context.runId}
            **Triggered by**: ${context.eventName}

            ## Failed Jobs
            ${{ toJson(needs) }}

            ## Investigation Steps
            - [ ] Review failed test logs
            - [ ] Check swarm coordination issues
            - [ ] Verify browser compatibility
            - [ ] Analyze performance regressions
            - [ ] Fix identified issues
            - [ ] Re-run failed tests

            ## Artifacts
            Check the workflow run for test results, screenshots, and logs.

            ---
            *This issue was automatically created by the test pipeline.*
            `;

            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['test-failure', 'automated', 'priority-high']
            });

      - name: Send Slack notification
        if: env.SLACK_WEBHOOK_URL
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: 'E2E Test Pipeline Failed'
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}